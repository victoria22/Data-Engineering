{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f271c980-14dc-476d-8162-9f3eebafa328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6072d6c4-7e30-4509-a4e2-679f0cd5113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 10:41:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fef560-2167-4ecc-a001-f70aabc3d386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 13:32:57--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.239.238.119, 18.239.238.212, 18.239.238.133, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.239.238.119|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61.36M  8.52MB/s    in 7.5s    \n",
      "\n",
      "2025-03-06 13:33:04 (8.20 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the parquet data\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b208382e-9bd9-4000-9bb1-bbbf1b10ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file into spark dataframe\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('yellow_tripdata_2024-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ce51c5-9c9c-43ef-9f20-63537595db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd4ec25-d103-49d6-9602-3b23e44400e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire Parquet file\n",
    "df = pd.read_parquet(\"yellow_tripdata_2024-10.parquet\")\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv(\"yellow_tripdata_2024-10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ae849c-7089-4d3e-95cd-02789ac50a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a sample data and store in a file called head\n",
    "!head -n 1001 yellow_tripdata_2024-10.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6aa22d-b2cb-4a5e-9149-7cce407ba262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the head file\n",
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1653960-169c-4f6b-a41c-db9a30bf093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                   int64\n",
       "tpep_pickup_datetime      object\n",
       "tpep_dropoff_datetime     object\n",
       "passenger_count          float64\n",
       "trip_distance            float64\n",
       "RatecodeID               float64\n",
       "store_and_fwd_flag        object\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "payment_type               int64\n",
       "fare_amount              float64\n",
       "extra                    float64\n",
       "mta_tax                  float64\n",
       "tip_amount               float64\n",
       "tolls_amount             float64\n",
       "improvement_surcharge    float64\n",
       "total_amount             float64\n",
       "congestion_surcharge     float64\n",
       "Airport_fee              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data types \n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70fd7c93-6655-4cc3-a94c-170986c44052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', StringType(), True), StructField('tpep_dropoff_datetime', StringType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', DoubleType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('Airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the schema using spark dataframe\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0a9119d-928e-4928-b770-d16776969fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76ad660e-7ffa-401f-9273-d12df34dc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema manually\n",
    "schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"passenger_count\", types.DoubleType(), True),\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.DoubleType(), True),\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"Airport_fee\", types.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d417b165-614e-44dd-9f93-909d3e50298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in the dataframe with the defined schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('yellow_tripdata_2024-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3b07eb9-95c0-4bf1-a47b-8ebb7d16dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file into partitions\n",
    "df = df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6167e1c-a8fd-4361-9edd-cc58e0da2170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the partitioned dataframe to parquet\n",
    "output_path = f'data/pq/yellow_tripdata_2024-10/'\n",
    "df.write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "654b6379-53ef-4189-a5cb-01a369778afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of partitions used \n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9dc03-58f3-4692-97fa-dd20d120e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the average size of the parquet file created\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the written Parquet files\n",
    "output_path = f'data/pq/yellow_tripdata_2024-10/'\n",
    "\n",
    "# Get a list of all Parquet files in the directory\n",
    "parquet_files = glob.glob(os.path.join(output_path, \"*.parquet\"))\n",
    "\n",
    "# Calculate total size of all Parquet files\n",
    "total_size_bytes = sum(os.path.getsize(f) for f in parquet_files)\n",
    "\n",
    "# Number of Parquet files (partitions)\n",
    "num_files = len(parquet_files)\n",
    "\n",
    "# Calculate average file size (convert from bytes to MB)\n",
    "average_size_mb = (total_size_bytes / num_files) / (1024 * 1024) if num_files > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Size: {total_size_bytes / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Number of Parquet Files: {num_files}\")\n",
    "print(f\"Average Parquet File Size: {average_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b74c8-1716-4e43-9073-d807cbf4e080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_env)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
